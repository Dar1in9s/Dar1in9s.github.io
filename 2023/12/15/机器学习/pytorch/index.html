<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="pytorch"><meta name="keywords" content=""><meta name="author" content="Dar1in9"><meta name="copyright" content="Dar1in9"><title>pytorch | Dar1in9's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.9.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"MY1GAADR9W","apiKey":"5032629da4e94f659d98a4052bc1c5cd","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor"><span class="toc-number">1.</span> <span class="toc-text">tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAtensor"><span class="toc-number">2.</span> <span class="toc-text">创建tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8tensor%E6%93%8D%E4%BD%9C"><span class="toc-number">3.</span> <span class="toc-text">常用tensor操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4tensor%E7%9A%84%E5%BD%A2%E7%8A%B6"><span class="toc-number">3.1.</span> <span class="toc-text">调整tensor的形状</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E6%88%96%E5%8E%8B%E7%BC%A9tensor%E7%BB%B4%E5%BA%A6"><span class="toc-number">3.2.</span> <span class="toc-text">添加或压缩tensor维度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E6%93%8D%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">索引操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E7%B4%A2%E5%BC%95"><span class="toc-number">4.1.</span> <span class="toc-text">高级索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch%E7%9A%84nn%E6%A8%A1%E5%9D%97"><span class="toc-number">5.</span> <span class="toc-text">PyTorch的nn模块</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://blog-1300147235.cos.ap-chengdu.myqcloud.com/kakashi-cat.jpg"></div><div class="author-info__name text-center">Dar1in9</div><div class="author-info__description text-center">人间值得，未来可期</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/Dar1in9s">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">89</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">17</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">17</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="https://gryffinbit.top">Gryffinbit</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://blog-1300147235.cos.ap-chengdu.myqcloud.com/Background2.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Dar1in9's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">pytorch</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2023-12-15</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 8 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>PyTorch是一个基于Python的科学计算包，它主要有两个用途：</p>
<p>1、类似Numpy但是能利用GPU加速<br>2、一个非常灵活和快速的用于深度学习的研究平台</p>
<span id="more"></span>


<h2 id="tensor"><a href="#tensor" class="headerlink" title="tensor"></a>tensor</h2><p>Tensor，也叫做张量，类似与NumPy的ndarray，但是可以用GPU加速。</p>
<p>而对于tensor的基础操作，可以从两个方面来讲。</p>
<p>一、如果从接口的角度，对一个tensor的操作可以分为两类：<br><code>torch.function</code>，如<code>torch.save</code><br><code>tensor.function</code>，如<code>tensor.view</code></p>
<blockquote>
<p>对于这两种接口方法，大多数时候都是等价的，如<code>torch.sum(a,b)</code>和<code>a.sum(b)</code></p>
</blockquote>
<p>二、如果从存储的角度讲，对tensor的操作也可以分为两类：<br><code>a.add(b)</code>，不会修改a自身的数据，加法的结果会返回一个新的tensor<br><code>a.add_(b)</code>，会修改a自身的数据，也就是说加法的结果存在a中</p>
<blockquote>
<p><strong>函数名以<code>_</code>结尾的都是修改调用者自身的数据。</strong></p>
</blockquote>
<h2 id="创建tensor"><a href="#创建tensor" class="headerlink" title="创建tensor"></a>创建tensor</h2><p>以下是一些创建的函数，其中如果<code>*size</code>为列表，则按照列表的形状生成张量，否则传入的参数看作是张量的形状：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Tensor(*size)	    <span class="comment"># 基础构造函数，默认为dtype(FloatTensor)类型</span></span><br><span class="line">tensor(data)        <span class="comment"># 根据传入的数据创建张量，会根据传入的值推测类型。</span></span><br><span class="line">ones(*sizes)	    <span class="comment"># 全1Tensor</span></span><br><span class="line">zeros(*sizes)	    <span class="comment"># 全0Tensor</span></span><br><span class="line">eye(*sizes)	        <span class="comment"># 对角矩阵（对角线为1，其他为0，不要求行列一致）</span></span><br><span class="line">arrange(s,e,step)	<span class="comment"># 从s到e，步长为step</span></span><br><span class="line">linspace(s,e,steps)	<span class="comment"># 从s到e，均匀分成steps份</span></span><br><span class="line">rand/randn(*sizes)	<span class="comment"># 均匀/标准分布</span></span><br><span class="line">normal(mean,std)/uniform(<span class="keyword">from</span>,tor)	<span class="comment"># 正态分布/均匀分布</span></span><br><span class="line">randperm(m)	        <span class="comment"># 随机排列</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>)      <span class="comment"># 指定形状构建2*3维的张量</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line">b = t.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])  <span class="comment"># 通过传入列表构建2*3维的张量，类型为自动推测</span></span><br><span class="line"><span class="comment"># tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#         [4, 5, 6]])</span></span><br><span class="line"></span><br><span class="line">b = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])   <span class="comment"># 通过传入列表构建2*3维的张量，类型为FloatTensor</span></span><br><span class="line"><span class="comment"># tensor([[1., 2., 3.],</span></span><br><span class="line"><span class="comment">#        [4., 5., 6.]])</span></span><br><span class="line"></span><br><span class="line">b.size()      <span class="comment"># 返回b的大小，等价于b.shape</span></span><br><span class="line">b.numel()     <span class="comment"># 计算b中的元素个数，等价于b.nelement()</span></span><br></pre></td></tr></table></figure>
<p><code>t.Tensor(*size)</code>创建tensor时，系统不会马上分配空间，只有使用到tensor时才会分配内存，而其他操作都是在创建tensor后马上进行空间分配。</p>
<h2 id="常用tensor操作"><a href="#常用tensor操作" class="headerlink" title="常用tensor操作"></a>常用tensor操作</h2><h3 id="调整tensor的形状"><a href="#调整tensor的形状" class="headerlink" title="调整tensor的形状"></a>调整tensor的形状</h3><p><code>view()</code>方法调整tensor的形状，但是<strong>必须得保证调整前后元素个数一致</strong>，但是view方法不会修改原tensor的形状和数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="comment"># # tensor([0, 1, 2, 3, 4, 5])</span></span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5]])</span></span><br><span class="line">  </span><br><span class="line">c = a.view(-<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># -1会自动计算大小。</span></span><br><span class="line"><span class="comment"># tensor([[0, 1],</span></span><br><span class="line"><span class="comment">#         [2, 3],</span></span><br><span class="line"><span class="comment">#        [4, 5]])</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">0</span>      <span class="comment"># view方法返回的tensor和原tensor共享内存，修改一个，另外一个也会修改</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;a: <span class="subst">&#123;a&#125;</span>\n\n c:<span class="subst">&#123;c&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># a: tensor([0, 0, 2, 3, 4, 5])</span></span><br><span class="line"><span class="comment"># c:tensor([[0, 0],</span></span><br><span class="line"><span class="comment">#         [2, 3],</span></span><br><span class="line"><span class="comment">#         [4, 5]])</span></span><br></pre></td></tr></table></figure>

<p><code>resize()</code>是另一种用来调整size的方法，但是它相比较<code>view</code>，可以修改tensor的尺寸，如果尺寸超过了原尺寸，则会自动分配新的内存，反之，则会保留老数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="comment"># tensor([0, 1, 2, 3, 4, 5])</span></span><br><span class="line"></span><br><span class="line">a.resize_(<span class="number">2</span>, <span class="number">3</span> )</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#        [3, 4, 5]])</span></span><br><span class="line"></span><br><span class="line">a.resize_(<span class="number">3</span>, <span class="number">3</span>)    <span class="comment"># 超过了的自动分配新的内存</span></span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0]])</span></span><br><span class="line"></span><br><span class="line">a.resize_(<span class="number">2</span>, <span class="number">2</span>)    <span class="comment"># 直接截取原数据</span></span><br><span class="line"><span class="comment"># tensor([[0, 1],</span></span><br><span class="line"><span class="comment">#         [2, 3]])</span></span><br></pre></td></tr></table></figure>


<h3 id="添加或压缩tensor维度"><a href="#添加或压缩tensor维度" class="headerlink" title="添加或压缩tensor维度"></a>添加或压缩tensor维度</h3><p><code>unsqueeze()</code>可以增加tensor的维度；<code>squeeze()</code>可以压缩tensor的维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"><span class="comment"># tensor([0, 1, 2, 3, 4, 5]) </span></span><br><span class="line"><span class="comment"># torch.Size([6])</span></span><br><span class="line"></span><br><span class="line">b = a.unsqueeze(<span class="number">1</span>)    <span class="comment"># 在第1维上增加“1”维，从6x0变成6x1</span></span><br><span class="line"><span class="comment"># tensor([[0],</span></span><br><span class="line"><span class="comment">#         [1],</span></span><br><span class="line"><span class="comment">#         [2],</span></span><br><span class="line"><span class="comment">#         [3],</span></span><br><span class="line"><span class="comment">#         [4],</span></span><br><span class="line"><span class="comment">#         [5]]) </span></span><br><span class="line"><span class="comment"># torch.Size([6, 1])</span></span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">a.resize_(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5]]) </span></span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">b = a.unsqueeze(<span class="number">0</span>)    <span class="comment"># 在第0维上增加“1”维，从2x3变成1x2x3</span></span><br><span class="line"><span class="comment"># tensor([[[0, 1, 2],</span></span><br><span class="line"><span class="comment">#          [3, 4, 5]]]) </span></span><br><span class="line"><span class="comment"># torch.Size([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">c = a.unsqueeze(-<span class="number">1</span>)   <span class="comment"># 在倒数第1维上增加“1”，也就是2*3的形状变成2*3*1。</span></span><br><span class="line"><span class="comment"># tensor([[[0],</span></span><br><span class="line"><span class="comment">#          [1],</span></span><br><span class="line"><span class="comment">#          [2]],</span></span><br><span class="line"><span class="comment">#         [[3],</span></span><br><span class="line"><span class="comment">#          [4],</span></span><br><span class="line"><span class="comment">#          [5]]]) </span></span><br><span class="line"><span class="comment"># torch.Size([2, 3, 1])</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">b = a.view(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[[0, 1, 2],</span></span><br><span class="line"><span class="comment">#          [3, 4, 5]]]) </span></span><br><span class="line"></span><br><span class="line">c = b.squeeze(<span class="number">0</span>)  <span class="comment"># 压缩第0维的“1”，某一维度为“1”才能压缩，如果第0维的维度是“2”如(2,1,3)则无法亚索第0维</span></span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5]]) </span></span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line"></span><br><span class="line">b.squeeze()  <span class="comment"># 把所有维度为“1”的压缩。</span></span><br></pre></td></tr></table></figure>

<h2 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h2><p>tensor的索引操作和ndarray的索引操作类似，并且索引出来的结果与原tensor<strong>共享内存</strong>。</p>
<h3 id="高级索引"><a href="#高级索引" class="headerlink" title="高级索引"></a>高级索引</h3><p>PyTorch 支持绝大多数 NumPy 的高级索引，高级索引可以看成是基本索引的扩展。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">9</span>).view([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5],</span></span><br><span class="line"><span class="comment">#        [6, 7, 8]])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[[<span class="number">0</span>, <span class="number">1</span>], ...])    <span class="comment"># 等价a[0]和a[1]，相当于索引张量的第一行和第二行元素；</span></span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5]])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>]]) <span class="comment"># 等价a[0, 1]和a[1, 2]，相当于索引张量的第一行的第二列和第二行的第三列元素；</span></span><br><span class="line"><span class="comment"># tensor([1, 5])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>], [<span class="number">0</span>]]) <span class="comment"># 等价a[1, 0]和a[0, 0]和a[2, 0]</span></span><br><span class="line"><span class="comment"># tensor([3, 0, 6])</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">index_select(<span class="built_in">input</span>,dim: _<span class="built_in">int</span>, index: Tensor)	<span class="comment"># 在指定维度dim上选取，例如选取某些行、某些列。输出结果与输入张量的维度相同。</span></span><br><span class="line">masked_select(<span class="built_in">input</span>, mask: Tensor)	            <span class="comment"># a.masked_select(a&gt;1)等价于a[a&gt;1] </span></span><br><span class="line">non_zero(<span class="built_in">input</span>)	                <span class="comment"># 获取非0元素的下标</span></span><br><span class="line">gather(<span class="built_in">input</span>,dim,index)	        <span class="comment"># 根据index，在dim维度上选取数据，输出的size与index一样</span></span><br><span class="line">take(index: Tensor)             <span class="comment"># 在原来Tensor的shape基础上打平，然后在打平后的Tensor上进行索引。</span></span><br></pre></td></tr></table></figure>

<p><strong>index_select</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">9</span>).view([<span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5],</span></span><br><span class="line"><span class="comment">#         [6, 7, 8]])</span></span><br><span class="line"></span><br><span class="line">b = a.index_select(<span class="number">0</span>, t.tensor([<span class="number">0</span>, <span class="number">2</span>]))    <span class="comment"># 选择第0维的0，2元素数据</span></span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [6, 7, 8]])</span></span><br></pre></td></tr></table></figure>

<p><strong>masked_select</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#        [3, 4, 5],</span></span><br><span class="line"><span class="comment">#        [6, 7, 8]])</span></span><br><span class="line"></span><br><span class="line">mask = a.gt(<span class="number">5</span>)    <span class="comment"># 生成a这个Tensor中大于5的元素的掩码</span></span><br><span class="line"><span class="comment"># tensor([[False, False, False],</span></span><br><span class="line"><span class="comment">#         [False, False, False],</span></span><br><span class="line"><span class="comment">#         [ True,  True,  True]])</span></span><br><span class="line"></span><br><span class="line">b = a.masked_select(mask)    <span class="comment"># 取出a这个Tensor中大于5的元素</span></span><br><span class="line"><span class="comment"># tensor([6, 7, 8])</span></span><br><span class="line"><span class="comment"># torch.Size([3])</span></span><br><span class="line"></span><br><span class="line">c = a[a&gt;<span class="number">5</span>]</span><br><span class="line"><span class="comment"># tensor([6, 7, 8])</span></span><br></pre></td></tr></table></figure>

<p><strong>take</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">9</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># tensor([[0, 1, 2],</span></span><br><span class="line"><span class="comment">#         [3, 4, 5],</span></span><br><span class="line"><span class="comment">#         [6, 7, 8]])</span></span><br><span class="line"></span><br><span class="line">b = a.take(t.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>]))</span><br><span class="line"><span class="comment"># tensor([0, 2, 5])</span></span><br></pre></td></tr></table></figure>

<h2 id="PyTorch的nn模块"><a href="#PyTorch的nn模块" class="headerlink" title="PyTorch的nn模块"></a>PyTorch的nn模块</h2><p>PyTorch有一个专门用于神经网络的完整子模块：<code>torch.nn</code>。该子模块包含创建各种神经网络体系结构所需的构建块。这些构建块在PyTorch术语中称为module（模块），在其他框架中称为layer（层）。</p>
<p>PyTorch模块都是从基类<code>nn.Module</code>继承而来的Python类。模块可以具有一个或多个参数（<code>Parameter</code>）实例作为属性，这些参数就是在训练过程中需要优化的张量（在之前的线性模型中即w和b）。模块还可以具有一个或多个子模块（<code>nn.Module</code>的子类）属性，并且也可以追踪其参数。</p>
<p>你可以毫不奇怪地可以找到一个名为<code>nn.Linear</code>的<code>nn.Module</code>子类，它对其输入进行仿射变换（通过参数属性weight和bias）；它就相当于之前在温度计实验中实现的方法。现在，从上次中断的地方开始，将之前的代码转换为使用<code>nn</code>的形式。</p>
<p>所有PyTorch提供的<code>nn.Module</code>子类都定义了其调用方法，使你可以实例化<code>nn.Linear</code>并将其像一个函数一样进行调用，如下面的代码所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">linear_model = nn.Linear(<span class="number">1</span>, <span class="number">1</span>) <span class="comment"># 参数: input size, output size, bias(默认True)</span></span><br><span class="line">linear_model(t_un_val)</span><br></pre></td></tr></table></figure>
<p>使用一组参数调用<code>nn.Module</code>实例最终会调用带有相同参数的名为<code>forward</code>的方法，<code>forward</code>方法会执行前向传播计算；不过在调用之前和之后还会执行其他相当重要的操作。因此，虽然从技术上讲直接调用<code>forward</code>是可行的，并且它产生的结果与调用<code>nn.Module</code>实例相同，但用户不应该这样做。</p>
<p><code>nn.Liner</code>表示线性模型（全连接层），<code>nn.Linear</code>的构造函数接受三个参数：输入特征的数量，输出特征的数量以及线性模型是否包含偏差（此处默认为True）。</p>
<p>这里特征的数量是指输入和输出张量的尺寸，因此本例是1和1。例如，如果在输入中同时使用了温度和气压，则在其中输入具有两个特征输入和而输出只有一个特征。如你所见，对于具有多个中间模块的更复杂的模型，模型的容量与特征的数量有关。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Dar1in9</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://dar1in9s.github.io/2023/12/15/机器学习/pytorch/">http://dar1in9s.github.io/2023/12/15/机器学习/pytorch/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://Dar1in9s.github.io">Dar1in9's Blog</a>！</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><i class="fa fa-chevron-left">  </i><span>卷积神经网络</span></a></div><div class="next-post pull-right"><a href="/2023/12/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/tensorflow/"><span>tensorflow</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://blog-1300147235.cos.ap-chengdu.myqcloud.com/Background2.png)"><div class="layout" id="footer"><div class="copyright">&copy;2020 - 2024 By Dar1in9</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.0"></script><script src="/js/fancybox.js?version=1.9.0"></script><script src="/js/sidebar.js?version=1.9.0"></script><script src="/js/copy.js?version=1.9.0"></script><script src="/js/fireworks.js?version=1.9.0"></script><script src="/js/transition.js?version=1.9.0"></script><script src="/js/scroll.js?version=1.9.0"></script><script src="/js/head.js?version=1.9.0"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>